# UniSA: Unified Generative Framework for Sentiment Analysis (ACM MM 2023)

#### **Zaijing Li**, **Ting-En Lin**, **Yuchuan Wu**, **Meng Liu**, **Fengxiao Tang**, **Ming Zhao**, **Yongbin Li**  

## Installation

1. Clone the repository recursively
    ```
    git clone https://github.com/dawn0815/UniSA.git
    ```

2. Create conda environment
    ```
    conda env create -f environment.yaml
    ```
## Datasets
In this paper, we use the proposed [SAEval Benchmark](https://github.com/dawn0815/SAEval-Benchmark) to train and evaluate our UniSA. The SAEval is a benchmark for sentiment analysis to evaluate the model's performance on various subtasks. All datasets were standardized to the same format and divided into training, validation and test sets. For more information about SAEval Benchmark, please refer to the [link](https://github.com/dawn0815/SAEval-Benchmark).

  
## Pre-train

#### Running Scripts

#### Pretrained Weights

## Fine-tune

#### Running Scripts

#### Finetuned Weights

## Few-shot

#### Datasets for Few-shot

#### Running Scripts

## Citing SAEval
If you use UniSA in your research, please use the following `bib` entry to cite the paper (paper link is coming soon).

## License
UniSA is released without any restrictions but restrictions may apply to individual tasks (which are derived from existing datasets) or backbone (e.g., GPT2, T5, and BART). We refer users to the original licenses accompanying each dataset and backbone.



